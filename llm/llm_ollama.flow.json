{
  "name": "llm_ollama",
  "type": "serial",
  "description": "Generate a chat completion with a provided model.",
  "input": {
    "schema": {
      "input": {
        "type": "object",
        "required": [
          "messages"
        ],
        "properties": {
          "model": {
            "type": "string",
            "description": "The model name.",
            "default": "llama2"
          },
          "uri": {
            "type": "string",
            "description": "The model endpoint.",
            "default": "http://localhost:11434/api/chat"
          },
          "messages": {
            "type": "array",
            "description": "A list of messages comprising the conversation so far.",
            "items": {
              "type": "object",
              "required": [
                "role",
                "content"
              ],
              "properties": {
                "role": {
                  "type": "string",
                  "description": "The role of the message author.",
                  "enum": [
                    "system",
                    "user",
                    "assistant"
                  ],
                  "default": "user"
                },
                "content": {
                  "type": "string",
                  "description": "The content of the message."
                }
              }
            }
          },
          "temperature": {
            "type": "number",
            "description": "What sampling temperature to use, between 0 and 2",
            "default": 0.8
          },
          "stream": {
            "type": "boolean",
            "description": "If set, partial message deltas will be sent. Tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a data: [DONE] message.",
            "default": true
          }
        }
      },
      "output": {
        "type": "object",
        "properties": {
          "content": {
            "type": "string",
            "description": "The message (or chunk if streamed) generated by the model."
          }
        }
      }
    },
    "tasks": [
      {
        "name": "chat",
        "type": "http",
        "input": {
          "body": {
            "model": "${input.model or \"llama2\"}",
            "messages": "${input.messages}",
            "options": {
              "temperature": "${0.8 if input.temperature == None else input.temperature}"
            },
            "stream": "${True if input.stream == None else input.stream}"
          },
          "method": "POST",
          "uri": "${input.uri or \"http://localhost:11434/api/chat\"}"
        }
      },
      {
        "name": "status",
        "type": "decision",
        "input": {
          "expression": "${str(chat.status)}",
          "cases": {
            "200": {
              "name": "stream",
              "type": "decision",
              "input": {
                "expression": "${str(isiterator(chat.body))}",
                "cases": {
                  "True": {
                    "name": "chunks",
                    "type": "terminate",
                    "input": {
                      "output": {
                        "iterator": "${chat.body}"
                      }
                    }
                  },
                  "False": {
                    "name": "response",
                    "type": "terminate",
                    "input": {
                      "output": "${chat.body.message}"
                    }
                  }
                }
              }
            }
          },
          "default": {
            "name": "failure",
            "type": "terminate",
            "input": {
              "error": "${chat.body.error}"
            }
          }
        }
      }
    ]
  }
}
