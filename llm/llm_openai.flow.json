{
  "name": "llm_openai",
  "type": "serial",
  "description": "Create a response for the given chat conversation from an LLM (compatible with OpenAI API)",
  "input": {
    "schema": {
      "input": {
        "type": "object",
        "required": [
          "messages"
        ],
        "properties": {
          "model": {
            "type": "string",
            "description": "ID of the model to use.",
            "default": "gpt-3.5-turbo"
          },
          "uri": {
            "type": "string",
            "description": "The model endpoint.",
            "default": "https://api.openai.com/v1/chat/completions"
          },
          "api_key": {
            "type": "string",
            "description": "The API key for authentication.",
            "default": "${getenv(\"OPENAI_API_KEY\")}"
          },
          "messages": {
            "type": "array",
            "description": "A list of messages comprising the conversation so far.",
            "items": {
              "type": "object",
              "properties": {
                "role": {
                  "type": "string",
                  "description": "The role of the messages author.",
                  "enum": [
                    "system",
                    "user",
                    "assistant"
                  ]
                },
                "content": {
                  "type": "string",
                  "description": "The contents of the user message."
                }
              }
            }
          },
          "temperature": {
            "type": "number",
            "description": "What sampling temperature to use, between 0 and 2",
            "default": 1
          },
          "stream": {
            "type": "boolean",
            "description": "If set, partial message deltas will be sent. Tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a data: [DONE] message."
          }
        }
      },
      "output": {
        "type": "object",
        "properties": {
          "response": {
            "type": "string",
            "description": "The response message."
          }
        }
      }
    },
    "tasks": [
      {
        "name": "chat",
        "type": "http",
        "input": {
          "body": {
            "model": "${input.model or \"gpt-3.5-turbo\"}",
            "messages": "${input.messages}",
            "temperature": "${input.temperature or 1}",
            "stream": "${input.stream}"
          },
          "header": {
            "Authorization": [
              "${\"Bearer \" + (input.api_key or getenv(\"OPENAI_API_KEY\"))}"
            ]
          },
          "method": "POST",
          "sse_filter": "${jsonencode({\"response\": jsondecode(data).choices[0].delta.content or \"\"})}",
          "uri": "${input.uri or \"https://api.openai.com/v1/chat/completions\"}"
        }
      },
      {
        "name": "status",
        "type": "decision",
        "input": {
          "expression": "${str(chat.status)}",
          "cases": {
            "200": {
              "name": "stream",
              "type": "decision",
              "input": {
                "expression": "${str(isiterator(chat.body))}",
                "cases": {
                  "True": {
                    "name": "chunks",
                    "type": "terminate",
                    "input": {
                      "output": {
                        "iterator": "${chat.body}"
                      }
                    }
                  },
                  "False": {
                    "name": "response",
                    "type": "terminate",
                    "input": {
                      "output": {
                        "response": "${chat.body.choices[0].message.content}"
                      }
                    }
                  }
                }
              }
            }
          },
          "default": {
            "name": "failure",
            "type": "terminate",
            "input": {
              "error": "${chat.body.error.message}"
            }
          }
        }
      }
    ]
  }
}
