{
  "name": "llm_azure_openai",
  "type": "serial",
  "description": "Create a response for the given chat conversation from an LLM (compatible with Azure OpenAI API).",
  "input": {
    "schema": {
      "input": {
        "type": "object",
        "required": [
          "resource_name",
          "deployment_name",
          "api_version",
          "messages"
        ],
        "properties": {
          "resource_name": {
            "type": "string",
            "description": "The name of your Azure OpenAI Resource."
          },
          "deployment_name": {
            "type": "string",
            "description": "The name of your model deployment. You're required to first deploy a model before you can make calls."
          },
          "api_version": {
            "type": "string",
            "description": "The API version to use for this operation. This follows the YYYY-MM-DD format.",
            "enum": [
              "2023-03-15-preview",
              "2023-05-15",
              "2023-06-01-preview",
              "2023-07-01-preview",
              "2023-08-01-preview"
            ]
          },
          "api_key": {
            "type": "string",
            "description": "The API key for authentication.",
            "default": "${getenv(\"AZURE_OPENAI_API_KEY\")}"
          },
          "messages": {
            "type": "array",
            "description": "A list of messages comprising the conversation so far.",
            "items": {
              "type": "object",
              "required": [
                "role",
                "content"
              ],
              "properties": {
                "role": {
                  "type": "string",
                  "description": "The role of the message author.",
                  "enum": [
                    "system",
                    "user",
                    "assistant"
                  ],
                  "default": "user"
                },
                "content": {
                  "type": "string",
                  "description": "The content of the message."
                }
              }
            }
          },
          "tools": {
            "type": "array",
            "description": "A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for.",
            "items": {
              "type": "object",
              "required": [
                "name"
              ],
              "properties": {
                "name": {
                  "type": "string",
                  "description": "The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."
                },
                "description": {
                  "type": "string",
                  "description": "A description of what the function does, used by the model to choose when and how to call the function."
                },
                "parameters": {
                  "$ref": "https://json-schema.org/draft/2020-12/schema",
                  "description": "The parameters the functions accepts, described as a JSON Schema object. See the guide for examples, and the JSON Schema reference for documentation about the format.\nOmitting parameters defines a function with an empty parameter list."
                }
              }
            }
          },
          "tool_choice": {
            "type": "string",
            "description": "Controls which (if any) function is called by the model. `none` means the model will not call a function and instead generates a message. `auto` means the model can pick between generating a message or calling a function. Specify a particular function name to force the model to call that function."
          },
          "temperature": {
            "type": "number",
            "description": "What sampling temperature to use, between 0 and 2",
            "default": 1
          },
          "stream": {
            "type": "boolean",
            "description": "If set, partial message deltas will be sent. Tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a data: [DONE] message."
          }
        }
      },
      "output": {
        "type": "object",
        "properties": {
          "content": {
            "type": "string",
            "description": "The message (or chunk if streamed) generated by the model."
          },
          "tool_calls": {
            "type": "array",
            "description": "The tool calls generated by the model, such as function calls.",
            "items": {
              "type": "object",
              "required": [
                "function"
              ],
              "properties": {
                "id": {
                  "type": "string",
                  "description": "The ID of the tool call."
                },
                "type": {
                  "type": "string",
                  "description": "The type of the tool. Currently, only `function` is supported."
                },
                "function": {
                  "type": "object",
                  "description": "The function that the model called.",
                  "required": [
                    "name",
                    "arguments"
                  ],
                  "properties": {
                    "name": {
                      "type": "string",
                      "description": "The name of the function to call."
                    },
                    "arguments": {
                      "type": "string",
                      "description": "The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."
                    }
                  }
                }
              }
            }
          }
        }
      }
    },
    "tasks": [
      {
        "name": "chat",
        "type": "http",
        "input": {
          "body": {
            "messages": "${input.messages}",
            "tools": "${None if not input.tools else [dict(type=\"function\", function=tool) for tool in input.tools]}",
            "tool_choice": "${input.tool_choice or None}",
            "temperature": "${1 if input.temperature == None else input.temperature}",
            "stream": "${input.stream}"
          },
          "header": {
            "api-key": [
              "${input.api_key or getenv(\"AZURE_OPENAI_API_KEY\")}"
            ]
          },
          "method": "POST",
          "sse_filter": "${jsonencode(jsondecode(data).choices[0].delta if jsondecode(data).choices else {})}",
          "uri": "https://${input.resource_name}.openai.azure.com/openai/deployments/${input.deployment_name}/chat/completions?api-version=${input.api_version}"
        }
      },
      {
        "name": "status",
        "type": "decision",
        "input": {
          "expression": "${str(chat.status)}",
          "cases": {
            "200": {
              "name": "stream",
              "type": "decision",
              "input": {
                "expression": "${str(isiterator(chat.body))}",
                "cases": {
                  "True": {
                    "name": "chunks",
                    "type": "terminate",
                    "input": {
                      "output": {
                        "iterator": "${chat.body}"
                      }
                    }
                  },
                  "False": {
                    "name": "response",
                    "type": "terminate",
                    "input": {
                      "output": "${chat.body.choices[0].message}"
                    }
                  }
                }
              }
            }
          },
          "default": {
            "name": "failure",
            "type": "terminate",
            "input": {
              "error": "${chat.body.error.message}"
            }
          }
        }
      }
    ]
  }
}
